---
title: "fuseMLR"
author: Cesaire J. K. Fouodo
output: 
  md_document:
    variant: gfm
    preserve_yaml: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<!-- badges: start -->
[![R-CMD-check](https://github.com/imbs-hl/fuseMLR/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/imbs-hl/fuseMLR/actions/workflows/R-CMD-check.yaml)
[![codecov](https://codecov.io/github/imbs-hl/fuseMLR/graph/badge.svg?token=SZU7NGK8G8)](https://codecov.io/github/imbs-hl/fuseMLR)
[![Lifecycle: Maturing](https://img.shields.io/badge/lifecycle-Maturing-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#Maturing)
[![CRAN downloads](http://www.r-pkg.org/badges/version/fuseMLR)](http://cranlogs.r-pkg.org/badges/grand-total/fuseMLR)
[![Stack Overflow](https://img.shields.io/badge/stackoverflow-questions-orange.svg)](https://stackoverflow.com/questions/tagged/fuseMLR)
<!-- badges: end -->

### fuseMLR
Cesaire J. K. Fouodo

### Introduction
Recent technological advances have enabled the simultaneous collection of multi-omics data, i.e., different types or modalities of molecular data across various organ tissues of patients. For integrative predictive modeling, the analysis of such data is particularly challenging. Ideally, data from the different modalities are measured in the same individuals, allowing for early or intermediate integrative techniques. However, they are often not applicable when patient data only partially overlap, which requires either reducing the datasets or imputing missing values. Additionally, the characteristics of each data modality may necessitate specific statistical methods rather than applying the same method across all modalities. Late integration modeling approaches analyze each data modality separately to obtain modality-specific predictions. These predictions are then integrated to train aggregative models like Lasso, random forests, or compute the weighted mean of modality-specific predictions. 

We introduce the R package fuseMLR for late integration prediction modeling. This comprehensive package enables users to define a training process with multiple data modalities and modality-specific machine learning methods. The package is user-friendly, facilitates variable selection and training of different models across modalities, and automatically performs aggregation once modality-specific training is completed. We simulated multi-omics data to illustrate the usage of our new package for conducting late-stage multi-omics integrative modeling.

`fuseMLR` is an object-oriented package based on `R6` version 2.5.1. Refer to our [cheat sheet](https://github.com/imbs-hl/fuseMLR/blob/master/README_files/figure-gfm/fusemlrcheatsheet.pdf) for a quick overview of classes and functionalities.

### Installation

Install the development version from GitHub with

```R
devtools::install_github("imbs-hl/fuseMLR")
```
### Package overview

The following figure illustrates the general architecture of `fuseMLR`:

```{r, echo=FALSE, out.width="70%", out.height="100%"}
knitr::include_graphics("README_files/figure-gfm/structure.png")
```

The general architecture of `fuseMLR` includes the collection classes `Training`, `TrainLayer`, and `TrainMetaLayer`. `TrainLayer` and `TrainMetaLayer` are stored within a `Training` instance, while `TrainData`, `Lrner`, and `VarSel` (for variable selection) are stored within a `TrainLayer` or `MetaTrainLayer` instance. An `Training` object can be used to automatically conduct layer-specific variable selection and train layer-specfic and meta models.

### Usage example

The following example is based on simulated data available in `fuseMLR`. Data have been simulated using the R package `InterSIM`, version 2.2.0. 

```{r libraries, warning = FALSE}
library(fuseMLR)
library(UpSetR)
library(ranger)
library(DescTools)
```

#### A) Simulated data.

Two types of data were simulated: training and testing datasets. Each consists of four `data.frame`sâ€”gene expression, protein expression, methylation data, and target variable observations. Individuals are organized in rows, variables in columns, with an additional column for individual IDs. In total, $70$ individuals with $50$ individuals pro layer have been simulated for training, and $23$ ($20$ per layer) for testing. Individuals do not necessarily overlapped. Effects have been introduced for gene expression and methylation by shifting the means by $0.5$ to create case-control study with $50$% prevalence. Individuals do not necessarily overlap. Effects were introduced in gene expression and methylation by shifting the means by 0.5 to create a case-control study. For illustration, the number of variables was kept smaller than what is typically expected in reality. The data simulation code is available [here](https://github.com/imbs-hl/fuseMLR/blob/master/test_code/build_data.R).


```{r data_exam, include=TRUE, eval=TRUE}
data("multi_omics")
# This is a list containing two lists of data: training and test.
# Each sublist contains three omics data.
str(object = multi_omics, max.level = 2L)
```

Variable selection, training and prediction are the main functionalities of `fuseMLR`. We can perform variable selection, train and fuse models for training studies, and predict new studies.

#### B) Instantiate training resources

We need to set up training resources.

```{r training, include=TRUE, eval=TRUE}
training <- createTraining(id = "training",
                           ind_col = "IDS",
                           target = "disease",
                           target_df = multi_omics$training$target,
                           verbose = TRUE)
print(training)
```

- Prepare training layers: Training layers contain `TrainData`, `Lrner` and `VarSel` objects. Therefore arguments to instantiate those object are provided when creating training layers. Users can choose to set up layer-specific learners, but for illustration, we will use the same learner for all layers. A `data.frame` of training data is required to instantiate a `TrainData` object. For `Lrner` and `VarSel` objects package and function names, and list of arguments are required.

```{r training_layers, include=TRUE, eval=TRUE}
# Create gene expression layer
createTrainLayer(training = training,
                 train_layer_id = "geneexpr",
                 train_data = multi_omics$training$geneexpr,
                 varsel_package = "Boruta",
                 varsel_fct = "Boruta",
                 varsel_param = list(num.trees = 1000L,
                                     mtry = 3L,
                                     probability = TRUE,
                                     na.action = "na.learn"),
                 lrner_package = "ranger",
                 lrn_fct = "ranger",
                 param_train_list = list(probability = TRUE,
                                         mtry = 1L,
                                     na.action = "na.learn"),
                 param_pred_list = list(),
                 na_action = "na.keep")

# Create gene protein abundance layer
createTrainLayer(training = training,
                 train_layer_id = "proteinexpr",
                 train_data = multi_omics$training$proteinexpr,
                 varsel_package = "Boruta",
                 varsel_fct = "Boruta",
                 varsel_param = list(num.trees = 1000L,
                                     mtry = 3L,
                                     probability = TRUE,
                                     na.action = "na.learn"),
                 lrner_package = "ranger",
                 lrn_fct = "ranger",
                 param_train_list = list(probability = TRUE,
                                         mtry = 1L,
                                     na.action = "na.learn"),
                 param_pred_list = list(),
                 na_action = "na.keep")

# Create methylation layer
createTrainLayer(training = training,
                 train_layer_id = "methylation",
                 train_data = multi_omics$training$proteinexpr,
                 varsel_package = "Boruta",
                 varsel_fct = "Boruta",
                 varsel_param = list(num.trees = 1000L,
                                     mtry = 3L,
                                     probability = TRUE,
                                     na.action = "na.learn"),
                 lrner_package = "ranger",
                 lrn_fct = "ranger",
                 param_train_list = list(probability = TRUE,
                                         mtry = 1L,
                                     na.action = "na.learn"),
                 param_pred_list = list(),
                 na_action = "na.keep")
```

- Also add a meta layer.

```{r training_meta_layers, include=TRUE, eval=TRUE}
# Create meta layer
createTrainMetaLayer(training = training,
                     meta_layer_id = "meta_layer",
                     lrner_package = NULL,
                     lrn_fct = "weightedMeanLearner",
                     param_train_list = list(),
                     param_pred_list = list(),
                     na_action = "na.impute")
```

- An upset plot of the training data: Visualize patient overlap across layers.

```{r upsetplot, include=TRUE, eval=TRUE, }
upsetplot(object = training, order.by = "freq")
```

#### C) Variable selection

Perform variable selection on our training resources

```{r varsel, include=TRUE, eval=TRUE}
# Variable selection
set.seed(5467)
var_sel_res <- varSelection(training = training)
print(var_sel_res)
```

For each layer, the variable selection results show the chosen variables. In this example, we perform variable selection. Users can opt to conduct variable selection on individual layers if desired.

#### D) Training

We can now train our models using the subset of selected variables.

```{r lrner_train, include=TRUE, eval=TRUE}
set.seed(5462)
training <- fusemlr(training = training,
                    use_var_sel = TRUE,
                    resampling_method = NULL,
                    resampling_arg = list(y = multi_omics$training$target$disease,
                                          k = 10L))

print(training)
# See also summary(training)
```

Use `extractModel` to retrieve the list of stored models and `extractData` to retrieve training data.

```{r basic_lrnr, include=TRUE, eval=TRUE}
models_list <- extractModel(training = training)
data_list <- extractData(training = training)
```

#### E) Predicting

In this section, we create a ```testing``` instance (from the *Testing* class) and make predictions for new data. This is done analogously to ```training```. The only difference that only the testing data modalities are required. Relevant functions are ```createTesting()``` and ```createTestLayer()```.

```{r testing, include=TRUE, eval=TRUE}
# Create testing for predcitions
testing <- createTesting(id = "testing",
                           ind_col = "IDS")

# Create gene expression layer
createTestLayer(testing = testing,
                test_layer_id = "geneexpr",
                 test_data = multi_omics$testing$geneexpr)

# Create gene protein abundance layer
createTestLayer(testing = testing,
                 test_layer_id = "proteinexpr",
                 test_data = multi_omics$testing$proteinexpr)

# Create methylation layer
createTestLayer(testing = testing,
                 test_layer_id = "methylation",
                 test_data = multi_omics$testing$proteinexpr)
```

- An upset plot of the training data: Visualize patient overlap across layers.

```{r upsetplot_new, include=TRUE, eval=TRUE, }
upsetplot(object = testing, order.by = "freq")
```

- Predict the testing object.

```{r new_pred, include=TRUE, eval=TRUE}
predictions <- predict(object = training, testing = testing)
print(predictions)
```

- Prediction performances for layer-specific available patients, and all patients on the meta layer.

```{r performance_all, include=TRUE, eval=TRUE}
pred_values <- predictions$predicted_values
actual_pred <- merge(x = pred_values,
                     y = multi_omics$testing$target,
                     by = "IDS",
                     all.y = TRUE)
x <- as.integer(actual_pred$disease == 2L)

# On all patients
perf_estimated <- sapply(X = actual_pred[ , 2L:5L], FUN = function (my_pred) {
  bs <- BrierScore(x = x[complete.cases(my_pred)],
                   pred = my_pred[complete.cases(my_pred)], na.rm = TRUE)
  return(bs)
})
print(perf_estimated)
```

- Prediction performances for overlapping individuals.

```{r performance_overlap, include=TRUE, eval=TRUE}
# On overlapping patients
perf_overlapping <- sapply(X = actual_pred[complete.cases(actual_pred),
                                           2L:5L],
                           FUN = function (my_pred) {
                             bs <- BrierScore(x = x[complete.cases(actual_pred)], pred = my_pred)
                             return(bs)
                           })
print(perf_overlapping)
```

Note that our example is based on simulated data for usage illustration; only one run is not enough to appreciate the performances of our models.

# E - Interface and wrapping #

We distinguish common supervised learning arguments from method specific arguments. The common arguments are a matrix ```x``` of independent variables and  ```y``` representing a response variable. These arguments are handled by ```fuseMLR```, so users do not need to make any adjustments themselves. We also define standard argument names for predicting. The arguments ```object``` and ```data``` (used by the generic ```R``` function ```predict``` to pass model and the data for which prediction are needed) are also stanard in ```fuseMLR```. If the original learning or predicting function do not use these names as arguments, either an interface or a wrapping of the original function can be done to solve name discepancies. 

## Interface ##

The interface approach leverages the arguments in ```createTrainLayer()``` to map the argument names of the original learning function. In the example below, the gene expression layer is re-created using the ```svm``` (Support Vector Machine) function from the ```e1071``` package as the learner. A discrepancy arises in the argument names of the ```predict.svm``` function, which uses ```object``` and ```newdata```.

```{r interface, include=TRUE, eval=TRUE}
# Re-create the gene expression layer with support vector machine as learner.
createTrainLayer(training = training,
                 train_layer_id = "geneexpr",
                 train_data = multi_omics$training$geneexpr,
                 varsel_package = "Boruta",
                 varsel_fct = "Boruta",
                 varsel_param = list(num.trees = 1000L,
                                     mtry = 3L,
                                     probability = TRUE),
                 lrner_package = "e1071",
                 lrn_fct = "svm",
                 param_train_list = list(type = 'C-classification',
                                         kernel = 'radial',
                                         probability = TRUE),
                 param_pred_list = list(probability = TRUE),
                 na_action = "na.keep",
                 x = "x",
                 y = "y",
                 object = "object",
                 data = "newdata", # Name discrepancy resolved.
                 extract_pred_fct = function (pred) { 
                   pred <- attr(pred, "probabilities")
                   return(pred[ , 1L])
                 }
)
# Variable selection
set.seed(5467)
var_sel_res <- varSelection(training = training)
set.seed(5462)
training <- fusemlr(training = training,
                    use_var_sel = TRUE)

print(training)
```

## Wrapping ##

In the wrapping approach we re-define the function ```mylasso``` to run a Lasso regression from the ```glmnet``` package as the meta-leaner.

```{r wrap, include=TRUE, eval=FALSE}
# We wrap the original functions
mylasso <- function (x, y,
                     nlambda = 25,
                     nfolds = 5) {
  # Perform cross-validation to find the optimal lambda
  cv_lasso <- cv.glmnet(x = as.matrix(x), y = y,
                        family = "binomial",
                        type.measure = "deviance",
                        nfolds = nfolds)
  best_lambda <- cv_lasso$lambda.min
  lasso_best <- glmnet(x = as.matrix(x), y = y,
                       family = "binomial",
                       alpha = 1,
                       lambda = best_lambda
  )
  lasso_model <- list(model = lasso_best)
  class(lasso_model) <- "mylasso"
  return(lasso_model)
}

predict.mylasso <- function(object, data) {
  glmnet_pred <- predict(object = object$model,
                         newx = as.matrix(data),
                         type = "response",
                         s = object$model$lambda)
  return(as.vector(glmnet_pred))
}

# Remove the current gene expression layer from training
removeLayer(training = training, layer_id = "meta_layer")
# Re-create the gene expression layer with support vector machine as learner.
createTrainMetaLayer(training = training,
                     meta_layer_id = "meta_layer",
                     lrner_package = NULL,
                     lrn_fct = "mylasso",
                     param_train_list = list(nlambda = 100L),
                     na_action = "na.impute")
set.seed(5462)
training <- fusemlr(training = training,
                    use_var_sel = TRUE)
print(training)
```

### Appendix

In addition to any pre-existing learner in R as a meta-learner, we have implemented the following ones.

```{r implemented_learners, include=TRUE, echo=FALSE}
# Load knitr package
library(knitr)

# Create a data frame
data <- data.frame(
  Leaner = c("weightedMeanLearner", "bestSpecificLearner"),
  Description = c("The weighted mean meta learner. It uses meta data to estimate the weights of the modality-specific models", "The best layer-specific model is used as meta model.")
)

# Generate the table
kable(data, caption = "")
```


&copy; 2024 Institute of Medical Biometry and Statistics (IMBS). All rights reserved.
